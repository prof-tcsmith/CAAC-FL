---
title: "Comparative Analysis of Aggregation Strategies in Federated Learning: A Multi-Dimensional Empirical Study"
author: "CAAC-FL Research Team"
date: today
format:
  html:
    toc: true
    toc-depth: 3
    toc-location: left
    number-sections: true
    code-fold: false
    theme: cosmo
    fig-width: 10
    fig-height: 6
    embed-resources: true
bibliography: references.bib
---

# Abstract

While federated learning research has extensively studied Byzantine-robust aggregation and non-IID data handling separately, the interaction between these challenges remains underexplored. We present a **multi-dimensional empirical study** comparing fundamental aggregation strategies—Federated Averaging (FedAvg), Federated Mean (FedMean), Federated Median (FedMedian), Standard Krum, and Multi-Krum—across 21 experimental configurations spanning three axes: **(1) data distribution heterogeneity** (IID-Equal vs. IID-Unequal), **(2) client scalability** (10, 25, 50 clients), and **(3) non-IID label skew** (Dirichlet α ∈ {0.1, 0.5, 1.0}). Through systematic experimentation on CIFAR-10, we provide the first empirical evidence that **Byzantine robustness and non-IID resilience require fundamentally different algorithmic approaches**.

Our key contributions and findings:

- **(1) Byzantine vs. Non-IID Distinction**: Standard Krum (designed for Byzantine attacks) fails catastrophically on non-IID data (**10.07%** at α=0.1, essentially random chance), while Multi-Krum succeeds (**60.69%**, a **+50.62 percentage point improvement**). This demonstrates that single-client selection—appropriate for excluding adversaries—fails where multi-client averaging succeeds for statistical heterogeneity.

- **(2) Aggregation Baselines**: FedAvg's dataset-size weighting provides +2.01% advantage over FedMean when client datasets vary in size, validating proportional weighting. FedMedian shows catastrophic failure under extreme non-IID (43.72% at α=0.1).

- **(3) Scalability Analysis**: Accuracy systematically degrades as client count increases (78.8% → 75.6% → 73.0% for FedAvg), confirming convergence challenges at scale.

- **(4) Non-IID Impact**: FedAvg drops from 72.98% (IID) to 66.77% (non-IID α=0.1), a −6.21% penalty for extreme heterogeneity.

These results establish empirical baselines for aggregation strategy selection and reveal a critical insight: methods designed for adversarial robustness (Krum) may fail under statistical heterogeneity (non-IID), while adapted variants (Multi-Krum) can succeed—informing practical FL system design where threat models differ from data distribution challenges.

**Keywords:** Federated Learning, Aggregation Strategies, Byzantine Robustness, Non-IID Data, Data Heterogeneity, Multi-Krum, CIFAR-10

---

# Introduction

## Federated Learning: A Privacy-Preserving Paradigm

Modern machine learning increasingly relies on vast quantities of data distributed across edge devices, mobile phones, and institutional boundaries. Traditional centralized approaches require aggregating raw data into a single location, raising critical concerns about privacy, data ownership, bandwidth consumption, and regulatory compliance [@mcmahan2017communication]. Federated Learning (FL) addresses these challenges by inverting the traditional training paradigm: rather than moving data to the model, FL brings the model to the data.

Introduced by McMahan et al. in 2017 [@mcmahan2017communication], federated learning enables multiple parties (clients) to collaboratively train a shared global model while keeping their data localized. Each client trains a local model on its private dataset, and only model updates (gradients or parameters) are communicated to a central server. The server aggregates these updates to produce an improved global model, which is then redistributed to clients for the next training round.

## The Critical Role of Aggregation

While federated learning's distributed training paradigm offers compelling advantages, it introduces unique challenges. Chief among these is the aggregation mechanism: the method by which the central server combines potentially heterogeneous local model updates from diverse clients. The choice of aggregation strategy profoundly impacts:

1. **Convergence speed**: How quickly the global model reaches optimal performance
2. **Final accuracy**: The ultimate predictive capability of the trained model
3. **Robustness**: Resilience to statistical heterogeneity, communication failures, and Byzantine (malicious) clients
4. **Computational efficiency**: The computational overhead imposed by the aggregation process

The seminal FedAvg algorithm [@mcmahan2017communication] employs weighted averaging, where client contributions are weighted proportionally to their dataset sizes. While effective and widely adopted, FedAvg assumes client honesty and may be vulnerable to adversarial manipulation. Alternative aggregation strategies—such as unweighted averaging and robust statistical estimators like median—offer different trade-offs in these dimensions.

## Research Gap and Motivation

While federated learning research has made significant progress on Byzantine-robust aggregation [@blanchard2017machine; @yin2018byzantine] and non-IID data handling [@li2020convergence; @zhao2018federated] separately, a critical gap remains: **the interaction between Byzantine-robust methods and non-IID data heterogeneity is poorly understood empirically**. Recent surveys [@wang2024noniid; @rodriguez2023aggregation] note that existing robust methods "may be broken in extreme non-IID scenarios," yet systematic empirical validation is lacking.

This gap is consequential because:

1. **Practical FL deployments face both challenges**: Real-world systems must handle statistical heterogeneity (honest but diverse clients) while potentially defending against adversaries
2. **Algorithmic assumptions differ**: Byzantine-robust methods assume honest clients have similar data; non-IID settings violate this assumption
3. **Design guidance is unclear**: Practitioners lack empirical evidence for choosing aggregation strategies when threat models and data distributions vary

This study addresses these gaps through:

1. **First empirical comparison of Standard Krum vs. Multi-Krum on non-IID data**, demonstrating that single-client selection fails catastrophically (10.07%) where multi-client averaging succeeds (60.69%)
2. **Multi-dimensional analysis** across three experimental axes: data heterogeneity, client scale, and non-IID severity
3. **Practical guidelines** distinguishing when Byzantine-robust methods are appropriate versus when non-IID-adapted variants are necessary

---

# Related Work

## Federated Learning Fundamentals

Federated Learning was introduced by McMahan et al. [@mcmahan2017communication] with the FedAvg algorithm, which employs weighted averaging of client model updates proportional to local dataset sizes. The theoretical convergence of FedAvg under non-IID conditions has been extensively studied by Li et al. [@li2020convergence], who established that data heterogeneity slows convergence and requires decaying learning rates. Zhao et al. [@zhao2018federated] provided early empirical evidence that non-IID data significantly degrades FedAvg performance, observing accuracy reductions of up to 55% compared to IID settings.

## Byzantine-Robust Aggregation

Byzantine fault tolerance in distributed learning was pioneered by Blanchard et al. [@blanchard2017machine], who proved that no linear combination of client updates can tolerate even a single Byzantine failure. They proposed Krum, which selects the single client update with minimal distance to its nearest neighbors. While theoretically sound for adversarial settings, subsequent research has revealed limitations. Fang et al. [@fang2020local] demonstrated that Krum and other Byzantine-robust methods remain vulnerable to sophisticated model poisoning attacks. Liu et al. [@liu2023mediankrum] proposed Median-Krum to address some of these limitations by combining distance-based and statistical robustness.

Critically, the interaction between Byzantine-robust aggregation and non-IID data heterogeneity remains underexplored. As noted in recent surveys [@wang2024noniid], "real-world training data used in FL are usually non-IID, which further weakens the robustness of existing FL methods such as Krum, Median, and Trimmed-Mean." This observation motivates our empirical investigation of Krum variants under controlled non-IID conditions.

## Non-IID Data Challenges and Solutions

The "client drift" phenomenon—where local updates diverge due to heterogeneous data distributions—has emerged as a central challenge in federated learning [@karimireddy2020scaffold]. Several algorithmic solutions have been proposed:

- **FedProx** [@li2020federated] adds a proximal regularization term to limit client drift
- **SCAFFOLD** [@karimireddy2020scaffold] uses control variates for variance reduction, demonstrating consistent improvements over FedAvg and FedProx
- **FedDC** [@gao2022feddc] decouples and corrects local drift, achieving state-of-the-art results on CIFAR-10 with Dirichlet-distributed non-IID data

While these methods represent important advances, they introduce additional complexity. Our study focuses on fundamental aggregation strategies to establish baselines and understand the underlying trade-offs before layering additional techniques.

## Empirical Comparison Studies

Several studies have compared federated learning aggregation algorithms. Rodriguez-Barroso et al. [@rodriguez2023aggregation] provide a comprehensive survey categorizing strategies by their contributions and limitations. Li et al. [@li2023experimental] conducted experimental studies of Byzantine-robust aggregation schemes. However, systematic comparisons across multiple dimensions—particularly contrasting Byzantine-robust methods (Krum) with their behavior on non-IID data—remain limited.

**Our Contribution**: This paper differs from prior work by (1) providing the first empirical demonstration that Standard Krum fails catastrophically on non-IID data while Multi-Krum succeeds, (2) analyzing aggregation strategies across three experimental axes simultaneously (heterogeneity, scale, non-IID), and (3) establishing the critical distinction between Byzantine robustness and non-IID resilience—different problems requiring different algorithmic solutions.

---

# Problem Statement

## Research Questions

This study investigates the following research questions:

**RQ1**: How do different aggregation strategies (weighted averaging, unweighted averaging, and median) compare in terms of convergence behavior and final model accuracy in federated learning with IID data?

**RQ2**: What is the computational and communication overhead associated with each aggregation strategy?

**RQ3**: What insights can be derived from IID baseline experiments to inform future research on Byzantine-robust aggregation?

## Hypotheses

Based on theoretical foundations and our multi-dimensional experimental design, we formulate the following hypotheses:

**H1** (FedAvg Weighting Advantage): *Federated Averaging will outperform Federated Mean when client datasets are heterogeneous in size, but perform similarly when dataset sizes are equal.*

The theoretical justification for FedAvg [@mcmahan2017communication] suggests that weighting updates by dataset size minimizes the global loss function. This advantage should be observable when clients have unequal dataset sizes (IID-Unequal), but diminish when all clients have equal amounts of data (IID-Equal). This hypothesis directly tests the value of proportional weighting.

**H2** (Scalability Degradation): *Model accuracy will decrease as the number of participating clients increases, due to increased gradient noise and slower convergence.*

With more clients, each client performs fewer local training steps per global dataset sample, potentially introducing more noise into the aggregated updates. We hypothesize that accuracy will systematically degrade when scaling from 10 → 25 → 50 clients, even under IID conditions.

**H3** (FedMedian Robustness Trade-off): *Federated Median will show slightly reduced convergence speed and final accuracy compared to averaging methods, representing a robustness-performance trade-off.*

Coordinate-wise median aggregation [@yin2018byzantine] discards information by selecting middle values rather than averaging, which may reduce statistical efficiency. However, this property provides inherent robustness to outliers.

---

# Methodology

## Experimental Framework

### Dataset

We utilize the CIFAR-10 dataset [@krizhevsky2009learning], a widely-adopted benchmark for image classification consisting of 60,000 32×32 color images across 10 classes (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck). The dataset comprises:

- **Training set**: 50,000 images (5,000 per class)
- **Test set**: 10,000 images (1,000 per class)

CIFAR-10's moderate complexity makes it suitable for federated learning experiments while remaining computationally tractable for systematic comparison studies.

### Data Partitioning

To comprehensively evaluate aggregation strategies across multiple dimensions, we employ three distinct partitioning strategies:

#### 1. IID-Equal (Baseline)

Random IID split with uniform client dataset sizes:

- Training data randomly shuffled and partitioned into equal disjoint subsets
- Each client receives equal samples: 5,000 (10 clients), 2,000 (25 clients), or 1,000 (50 clients)
- All clients share the same test set (10,000 images) for centralized evaluation
- Each client has approximately uniform class distribution

This baseline ensures client datasets are statistically homogeneous in both label distribution and size.

#### 2. IID-Unequal (Dataset Size Heterogeneity)

Random IID split with heterogeneous client dataset sizes:

- Training data randomly shuffled (preserving IID label distribution)
- Client dataset sizes sampled from Dirichlet distribution: $\text{Dir}(\alpha \mathbf{1})$ with $\alpha = 2.0$
- Dataset sizes vary from ~500 to ~1,500 samples per client (tested with 50 clients)
- Size heterogeneity parameter: 0.5 (moderate variation)

This partitioning isolates the effect of FedAvg's dataset-size weighting by maintaining IID labels while varying client data quantities.

#### 3. Non-IID (Label Distribution Heterogeneity)

Dirichlet-distributed label skew with equal client sizes:

- Each client's label distribution sampled from Dirichlet: $\text{Dir}(\alpha \mathbf{1})$
- Concentration parameter: $\alpha \in \{0.1, 0.5, 1.0\}$
  - $\alpha = 0.1$: Extreme heterogeneity (clients have 1-2 dominant classes)
  - $\alpha = 0.5$: Moderate heterogeneity
  - $\alpha = 1.0$: Mild heterogeneity (approaching IID)
- Equal client dataset sizes (1,000 samples each for 50 clients)

This partitioning tests robustness to statistical heterogeneity while controlling for dataset size effects.

### Model Architecture

We employ a SimpleCNN architecture suitable for CIFAR-10:

```python
SimpleCNN(
  Conv2d(3, 32, kernel_size=3, padding=1)
  ReLU()
  Conv2d(32, 64, kernel_size=3, padding=1)
  ReLU()
  MaxPool2d(kernel_size=2, stride=2)

  Conv2d(64, 128, kernel_size=3, padding=1)
  ReLU()
  MaxPool2d(kernel_size=2, stride=2)

  Fully Connected(128 × 8 × 8 → 256)
  ReLU()
  Dropout(0.5)
  Fully Connected(256 → 10)
)
```

**Total parameters**: 61,322

The architecture balances expressiveness with computational efficiency, enabling rapid experimentation across multiple aggregation strategies.

### Experimental Design

We conduct 18 experiments across three experimental dimensions:

**Dimension 1: Data Distribution**

- IID-Equal (baseline)
- IID-Unequal (dataset size heterogeneity)
- Non-IID with α ∈ {0.1, 0.5, 1.0} (label skew)

**Dimension 2: Client Count**

- 10 clients (small-scale)
- 25 clients (medium-scale)
- 50 clients (large-scale)

**Dimension 3: Aggregation Strategy**

- FedAvg (weighted averaging)
- FedMean (unweighted averaging)
- FedMedian (coordinate-wise median)
- Standard Krum (m=1, single-client selection, Non-IID experiments only)
- Multi-Krum (m=48, multi-client averaging, Non-IID experiments only)

### Training Configuration

| Parameter | Value |
|-----------|-------|
| **Federated Learning** | |
| Communication rounds | 20 |
| Clients per round | 100% participation |
| Client selection | All clients selected each round |
| | |
| **Local Training** | |
| Local epochs | 5 |
| Batch size | 32 |
| Optimizer | SGD with momentum (0.9) |
| Learning rate | 0.01 |
| Weight decay | 0.0 |
| | |
| **Infrastructure** | |
| Framework | Flower 1.7.0 |
| Simulation backend | Ray 2.9.0 |
| Hardware | 2× NVIDIA RTX 4090 (24GB each) |
| CPU cores | 128 |
| GPU allocation | 0.04 per client |
| Random seed | 42 (reproducibility) |

**Note on Communication Rounds**: We use 20 rounds (reduced from typical 50-100) to enable rapid iteration across 18 experimental configurations. While this represents a trade-off between convergence completeness and experimental throughput, our results demonstrate clear convergence trends and hypothesis validation within this window.

### Aggregation Strategies

#### 1. Federated Averaging (FedAvg)

The standard weighted averaging approach [@mcmahan2017communication]:

$$
\mathbf{w}_{t+1} = \sum_{i=1}^{N} \frac{n_i}{n} \mathbf{w}_{t+1}^{i}
$$

where:

- $\mathbf{w}_{t+1}$ is the global model at round $t+1$
- $\mathbf{w}_{t+1}^{i}$ is the local model from client $i$
- $n_i$ is the number of samples at client $i$
- $n = \sum_{i=1}^{N} n_i$ is the total number of samples

**Characteristics**: Optimal under IID conditions with honest clients; weights reflect dataset sizes.

#### 2. Federated Mean (FedMean)

Unweighted averaging treating all clients equally:

$$
\mathbf{w}_{t+1} = \frac{1}{N} \sum_{i=1}^{N} \mathbf{w}_{t+1}^{i}
$$

where $N$ is the number of clients.

**Characteristics**: Simpler aggregation; equivalent to FedAvg when all clients have equal dataset sizes (as in our IID setup).

#### 3. Federated Median (FedMedian)

Coordinate-wise median aggregation [@yin2018byzantine]:

$$
\mathbf{w}_{t+1}[j] = \text{median}\{\mathbf{w}_{t+1}^{1}[j], \mathbf{w}_{t+1}^{2}[j], \ldots, \mathbf{w}_{t+1}^{N}[j]\}
$$

for each parameter $j$ in the model.

**Characteristics**: Robust to outliers; discards extreme values; higher computational cost ($O(N \log N)$ per parameter vs. $O(N)$ for averaging).

#### 4. Krum and Multi-Krum

The Krum algorithm [@blanchard2017machine] was designed for Byzantine-robust aggregation. We evaluate two variants to empirically demonstrate their different behaviors on Non-IID data:

**Krum Score Computation** (shared by both variants):

1. For each client $i$, compute pairwise distances: $d_{ij} = \|\mathbf{w}_{t+1}^{i} - \mathbf{w}_{t+1}^{j}\|_2$
2. For each client $i$, compute Krum score: sum of distances to $k = n - f - 2$ nearest neighbors
3. Rank clients by score (lower = more "typical" update)

**Standard Krum** ($m = 1$):

Select the single client with the lowest score and use its update directly:

$$
\mathbf{w}_{t+1} = \mathbf{w}_{t+1}^{i^*} \quad \text{where} \quad i^* = \arg\min_i \text{score}(i)
$$

**Characteristics**: Designed for Byzantine fault tolerance assuming honest clients have similar (IID) data. Fails on Non-IID data because selecting one specialized client cannot represent the full data distribution.

**Multi-Krum** ($m > 1$):

Select top $m$ clients with lowest scores and **average** their updates:

$$
\mathbf{w}_{t+1} = \frac{1}{m} \sum_{i \in S} \mathbf{w}_{t+1}^{i} \quad \text{where} \quad S = \{m \text{ clients with lowest scores}\}
$$

**Parameters**: For our Non-IID experiments with $n=50$ clients and $f=0$ Byzantine clients:

- **Standard Krum**: $m = 1$ (single-client selection)
- **Multi-Krum**: $m = n - f - 2 = 48$ (excluding only 2 worst outliers)

**Characteristics**: Multi-Krum preserves Byzantine robustness (still excludes outliers) while capturing diverse data distributions through multi-client averaging—essential for Non-IID settings.

## Evaluation Metrics

We evaluate aggregation strategies across multiple dimensions:

### 1. Test Accuracy
Global model accuracy on the centralized test set after each communication round, measuring the model's generalization capability.

### 2. Test Loss
Cross-entropy loss on the test set, providing a continuous measure of model quality and convergence.

### 3. Convergence Speed
Number of communication rounds required to achieve specific accuracy thresholds (e.g., 70%, 75%).

### 4. Final Performance
Test accuracy and loss after 50 communication rounds, representing the ultimate quality of the trained model.

---

# Results

We present results from 18 experimental configurations across three dimensions: data distribution (IID-Equal, IID-Unequal, Non-IID), client count (10, 25, 50), and aggregation strategy (FedAvg, FedMean, FedMedian, Multi-Krum). This section focuses on Level 1 experiments (IID data with varying client counts and dataset sizes), which directly test our hypotheses H1-H3.

## Summary of All Experiments

```{python}
#| echo: false
#| label: tbl-comprehensive
#| tbl-cap: "Final test accuracy (%) across all Level 1 experimental configurations after 20 communication rounds."

import pandas as pd

# Load comprehensive summary
data = pd.read_csv('comprehensive_summary.csv')

# Sort by partition, clients, aggregation for readability
data_sorted = data.sort_values(['Partition', 'Clients', 'Aggregation'])

print(data_sorted.to_markdown(index=False))
```

@tbl-comprehensive presents the complete results across all experimental conditions. Key observations:

1. **Best Performance**: FedAvg with 10 clients (IID-Equal) achieves 78.84% accuracy
2. **Worst Performance**: FedMedian with 50 clients (IID-Unequal) achieves 68.27% accuracy
3. **Performance Range**: All methods fall within an 8-11% accuracy range
4. **Consistent Pattern**: Accuracy decreases with increased client count across all methods

## H1: FedAvg Weighting Advantage (IID-Equal vs IID-Unequal)

![IID-Equal vs. IID-Unequal Comparison](comparison_iid_equal_vs_unequal.png)

**Figure 1**: Convergence comparison between IID-Equal (equal client dataset sizes) and IID-Unequal (heterogeneous client sizes) partitioning strategies across all three aggregation methods with 50 clients over 20 rounds.

The left panel shows convergence trajectories, while the right panel displays final accuracy comparisons. Key findings:

**IID-Unequal (50 clients, heterogeneous sizes)**:

- **FedAvg**: 72.98% (highest)
- **FedMean**: 70.97%
- **FedMedian**: 68.27%

**FedAvg Advantage**: **+2.01%** over FedMean (72.98% vs 70.97%)

This validates **H1**: FedAvg's dataset-size weighting provides measurable advantage when client datasets differ in size. The +2.01% improvement demonstrates that proportional weighting is not merely a theoretical nicety—it materially improves model quality under realistic heterogeneous dataset size conditions.

**Why FedMean Underperforms with Unequal Sizes**: FedMean treats all clients equally regardless of dataset size. Clients with 500 samples receive the same weight as clients with 1,500 samples, effectively underweighting data-rich clients and overweighting data-poor clients. This introduces bias in the aggregated update.

## H2: Scalability Across Client Counts

![Client Scaling Analysis](comparison_client_scaling.png)

**Figure 2**: Performance degradation as client count increases from 10 → 25 → 50 for IID-Equal partitioning. Left panel shows convergence curves for all three aggregation methods; right panel shows final accuracy vs. client count.

**FedAvg Scalability**:

- **10 clients**: 78.84% (baseline)
- **25 clients**: 75.58% (−3.26%)
- **50 clients**: 72.98% (−5.86% from baseline)

**Systematic Degradation**: Accuracy consistently decreases as client count increases, confirming **H2**. This pattern holds across all aggregation strategies:

| Aggregation | 10 Clients | 25 Clients | 50 Clients | Degradation |
|-------------|-----------|-----------|-----------|-------------|
| FedAvg | 78.84% | 75.58% | 72.98% | −5.86% |
| FedMean | 78.47% | 75.69% | 70.97% | −7.50% |
| FedMedian | 78.54% | 75.02% | 68.27% | −10.27% |

**Why Scalability Degrades**: With more clients, each client's local training occurs on a smaller fraction of the global dataset. This increases gradient noise in aggregated updates and slows convergence. With fixed communication rounds (20), systems with more clients have less time to converge.

**FedMedian Scalability Penalty**: FedMedian suffers the largest degradation (−10.27%), suggesting that coordinate-wise median is particularly sensitive to increased gradient noise at scale.

## H3: FedMedian Robustness-Performance Trade-off

Across all experimental conditions, FedMedian consistently trails averaging methods:

**Performance Gap**:

- vs. FedAvg: 0.3% to 4.71% lower accuracy (depending on configuration)
- vs. FedMean: 0.07% to 2.7% lower accuracy

**Convergence Speed**: Visual inspection of convergence curves (Figures 1-2) shows FedMedian exhibits slower initial convergence, particularly in rounds 5-15.

**Theoretical Robustness**: While we do not test Byzantine scenarios here, FedMedian's coordinate-wise median provides inherent resistance to outliers with a breakdown point of 50%. This robustness comes at the cost of statistical efficiency—discarding information from extreme (but potentially honest) updates.

**Verdict**: **H3 Confirmed**. FedMedian demonstrates the expected robustness-performance trade-off.

## Extended Convergence Analysis (20 vs 50 Rounds)

To validate that patterns observed at 20 rounds persist at full convergence, we extended two critical IID configurations to 50 communication rounds: IID-Unequal (tests H1 weighted averaging advantage) and IID-Equal 50 clients (baseline). These experiments reveal substantial ongoing improvement beyond 20 rounds.

### Convergence Patterns

![Extended Convergence Comparison](convergence_20_vs_50_rounds.png)

**Figure 3**: Convergence trajectory comparison between 20-round (solid lines) and 50-round (dashed lines) experiments for IID-Unequal (left panel) and IID-Equal (right panel) with 50 clients. All three aggregation strategies show substantial improvement beyond 20 rounds.

**Key Findings**:

| Configuration | Aggregation | 20 Rounds | 50 Rounds | Improvement | % of Final |
|--------------|-------------|-----------|-----------|-------------|-----------|
| **IID-Unequal** | FedAvg | 72.98% | 77.68% | +4.70% | 93.9% |
| | FedMean | 70.97% | 77.47% | +6.50% | 91.6% |
| | FedMedian | 68.27% | 75.23% | +6.96% | 90.7% |
| **IID-Equal** | FedAvg | 70.75% | 77.07% | +6.32% | 91.8% |
| | FedMean | 70.44% | 77.17% | +6.73% | 91.3% |
| | FedMedian | 69.16% | 75.90% | +6.74% | 91.1% |

**Convergence Incompleteness**: Models at 20 rounds have reached only 90.7-93.9% of their 50-round performance, demonstrating that they have **NOT** fully converged. All strategies continue improving substantially with additional communication.

**FedMedian Slower Convergence**: FedMedian shows the largest improvement gaps (+6.74% to +6.96%), indicating it converges more slowly than averaging methods. At 20 rounds, FedMedian reaches only ~91% of its final performance, compared to ~94% for FedAvg.

**Why FedMedian Converges Slowly**: Coordinate-wise median discards information by selecting middle values rather than averaging all updates. This statistical inefficiency slows learning, particularly in early rounds when gradient signals are strongest. Extended training allows FedMedian to eventually reach performance comparable to FedAvg (within 1.78% at 50 rounds in IID-Unequal, vs. 4.71% gap at 20 rounds).

**Hypothesis Validation at Convergence**:

Despite substantial accuracy improvements, the relative rankings and hypothesis validations remain stable:

- **H1 (FedAvg Advantage)**: Confirmed at both 20r and 50r
  - 20 rounds: FedAvg +2.01% over FedMean (IID-Unequal)
  - 50 rounds: FedAvg +0.21% over FedMean (IID-Unequal)
  - Note: Gap narrows but direction remains consistent

- **H3 (FedMedian Trade-off)**: Confirmed at both 20r and 50r
  - FedMedian consistently trails averaging methods
  - 20 rounds: FedAvg leads FedMedian by 4.71% (IID-Unequal)
  - 50 rounds: FedAvg leads FedMedian by 2.45% (IID-Unequal)
  - Gap narrows as FedMedian catches up, but performance order stable

### Implications for Experimental Design

**Communication-Accuracy Trade-off**: Our findings reveal a critical trade-off:

- **20 rounds**: Achieves 90-94% of final accuracy with ~40% of communication cost
- **50 rounds**: Required for full convergence and accurate final performance comparison

**Practical Recommendations**:

1. **Early-stage research/prototyping**: 20 rounds sufficient for rapid hypothesis testing
2. **Final evaluation/deployment**: 50+ rounds necessary for accurate performance assessment
3. **FedMedian evaluation**: Requires longer training to reach competitive performance
4. **Convergence detection**: Monitor validation loss plateau rather than fixed rounds

### Convergence Rate Analysis

**Rounds to Reach 75% Accuracy** (IID-Unequal):

| Aggregation | Rounds to 75% | Relative Speed |
|-------------|---------------|----------------|
| FedAvg | ~35 rounds | Fastest |
| FedMean | ~40 rounds | Moderate |
| FedMedian | ~47 rounds | Slowest |

FedAvg reaches 75% accuracy ~12 rounds sooner than FedMedian, representing a 25% reduction in communication overhead for the same accuracy target.

## Level 2: Non-IID Performance Analysis

### Non-IID Data Heterogeneity Impact

![Non-IID Comprehensive Analysis](noniid_comprehensive_analysis.png)

**Figure 4**: Non-IID performance analysis. Left: Convergence trajectories for extreme heterogeneity (α=0.1). Right: Final accuracy across heterogeneity levels (α ∈ {0.1, 0.5, 1.0}) for FedAvg and FedMedian.

Level 2 experiments introduce label distribution skew using Dirichlet-distributed client data with concentration parameter α. Lower α values indicate more extreme heterogeneity:

- **α = 0.1**: Extreme heterogeneity (clients have 1-2 dominant classes)
- **α = 0.5**: Moderate heterogeneity
- **α = 1.0**: Mild heterogeneity (approaching IID)

**Key Findings**:

| Aggregation | α=0.1 | α=0.5 | α=1.0 | Degradation (0.1→1.0) |
|-------------|-------|-------|-------|----------------------|
| FedAvg | 66.77% | 69.62% | 70.49% | +3.72% |
| FedMedian | 43.72% | 63.79% | 67.19% | +23.47% |

**Non-IID Penalty (vs. IID-Unequal baseline)**:

- FedAvg: 72.98% (IID) → 66.77% (α=0.1) = **−6.21% penalty**
- FedMedian: 68.27% (IID) → 43.72% (α=0.1) = **−24.55% penalty**

### FedMedian Catastrophic Failure

FedMedian exhibits catastrophic performance degradation under extreme Non-IID conditions (α=0.1), achieving only 43.72% accuracy—barely better than random chance for a 10-class problem (10% baseline). This represents:

- **−24.55% drop** from IID-Unequal performance
- **−23.05% gap** vs. FedAvg under same conditions
- Complete loss of competitive advantage

**Root Cause**: Coordinate-wise median is highly sensitive to statistical heterogeneity. When clients have non-overlapping label distributions:

1. Model parameters diverge significantly across clients
2. Median operation selects parameters from potentially incompatible models
3. Resulting global model fails to generalize
4. Convergence stalls or diverges

**Implication**: FedMedian's robustness to Byzantine attacks comes at the cost of extreme fragility to statistical heterogeneity—paradoxically making it unsuitable for realistic federated scenarios where data heterogeneity is the primary challenge.

### IID vs. Non-IID Direct Comparison

![IID vs Non-IID Comparison](iid_vs_noniid_comparison.png)

**Figure 5**: Direct comparison of IID-Equal (left) vs. Non-IID α=0.5 (right) performance for FedAvg and FedMedian with 50 clients.

The side-by-side comparison reveals:

**FedAvg Resilience**:

- IID-Equal (50 clients): 72.98%
- Non-IID α=0.5: 69.62%
- Degradation: −3.36% (relatively robust)

**FedMedian Brittleness**:

- IID-Equal (50 clients): 68.27%
- Non-IID α=0.5: 63.79%
- Degradation: −4.48% (more sensitive)

FedAvg demonstrates superior resilience to data heterogeneity, maintaining reasonable performance even under moderate Non-IID conditions. FedMedian's sensitivity suggests it should be avoided in heterogeneous federated settings.

### Krum Analysis: Standard vs. Multi-Krum on Non-IID Data

We conducted a systematic comparison of two Krum variants to empirically demonstrate why single-client selection fails on heterogeneous data:

- **Standard Krum** (m=1): Original algorithm selecting single client with lowest Krum score
- **Multi-Krum** (m=48): Extension averaging top m=n−f−2 clients

![Standard Krum vs Multi-Krum Comparison](krum_standard_vs_multi_comparison.png)

**Figure 7**: Left panel shows accuracy comparison across all aggregation methods for each heterogeneity level. Right panel shows the dramatic improvement (+27% to +51%) Multi-Krum achieves over Standard Krum.

![Krum Convergence Comparison](krum_convergence_comparison.png)

**Figure 8**: Convergence trajectories comparing Standard Krum (red) vs Multi-Krum (green) across three heterogeneity levels. Standard Krum remains near random chance (10%) throughout training at α=0.1, while Multi-Krum converges to 60%+ accuracy.

**Comprehensive Comparison (20 rounds, 50 clients)**:

| Dirichlet α | KL Divergence | Standard Krum | Multi-Krum | FedAvg | FedMedian | Multi vs Std |
|-------------|---------------|---------------|------------|--------|-----------|--------------|
| **0.1** (extreme) | 1.3950 | **10.07%** | **60.69%** | 66.77% | 43.72% | **+50.62%** |
| **0.5** (moderate) | 0.5898 | **31.93%** | **67.12%** | 69.62% | 63.79% | **+35.19%** |
| **1.0** (mild) | 0.3330 | **42.14%** | **69.56%** | 70.49% | 67.19% | **+27.42%** |

**Critical Findings**:

1. **Standard Krum Catastrophic Failure**: At extreme heterogeneity (α=0.1), standard Krum achieves only **10.07%** accuracy—essentially random chance for 10-class classification
2. **Multi-Krum Dramatic Improvement**: Multi-Krum improves accuracy by **+50.62 percentage points** over standard Krum at α=0.1
3. **Heterogeneity Correlation**: Standard Krum's failure is directly proportional to heterogeneity severity (10% → 32% → 42% as α increases)
4. **Multi-Krum Outperforms FedMedian**: Across all heterogeneity levels, Multi-Krum beats coordinate-wise median (+2.37% to +16.97%)

**Why Standard Krum Fails on Non-IID Data**:

Standard Krum selects a **SINGLE** client's model update based on distance scores. With extreme Non-IID (α=0.1), each client specializes in only 1-2 classes out of 10:

- Client A has primarily classes {0, 1} → trained model excels at {0, 1}
- Client B has primarily classes {2, 3} → trained model excels at {2, 3}
- Krum selects Client A (lowest distance to neighbors)
- **Result**: Global model only performs well on ~10% of classes → 10.07% test accuracy

The selected client's model is locally optimal but globally catastrophic because it lacks information about 80-90% of the label space.

**Why Multi-Krum Succeeds**:

Multi-Krum selects and **averages** top m clients with lowest Krum scores:

- m = n − f − 2 = 50 − 0 − 2 = 48 clients selected
- Averaging 48 clients captures information from nearly all class distributions
- Only 2 worst outliers excluded (preserving Byzantine robustness)
- **Result**: Global model aggregates knowledge across full label space → 60.69% accuracy

**Byzantine vs. Non-IID: Critical Algorithmic Distinction**:

| Threat Model | Goal | Optimal Strategy | Client Selection |
|--------------|------|------------------|------------------|
| **Byzantine Attacks** | Exclude malicious updates | Standard Krum | Single most trustworthy |
| **Non-IID Heterogeneity** | Aggregate diverse honest updates | Multi-Krum | Average multiple diverse |

Standard Krum was designed for Byzantine fault tolerance where the assumption is that honest clients have similar data (IID). Multi-Krum adapts the algorithm for statistical heterogeneity where diversity is a feature, not a bug. This reveals a fundamental insight: **Byzantine robustness and Non-IID resilience require different algorithmic approaches**.

## Grand Comparison Heatmap

![Grand Comparison Heatmap - All Experiments](grand_heatmap_all_experiments.png)

**Figure 6**: Comprehensive heatmap of final test accuracy across ALL Level 1 and Level 2 experiments. Rows represent experimental configurations (data type and client count); columns represent aggregation methods. Multi-Krum results included for Non-IID experiments.

The comprehensive heatmap provides a holistic view of all 15 successful experiments:

**Key Patterns**:

- **IID Experiments (top rows)**: Warm colors (75-79% accuracy) for small-scale scenarios (10, 25 clients)
- **IID-Unequal (middle rows)**: Moderate cooling (68-73% accuracy) with clear FedAvg advantage
- **Non-IID Experiments (bottom rows)**: Significant cooling (44-70% accuracy) with catastrophic FedMedian failure at α=0.1
- **Column Comparison**: FedAvg consistently outperforms across all conditions; FedMedian shows extreme variability
- **Diagonal Degradation**: Performance systematically degrades from top-left (best: IID-Equal, 10 clients) to bottom-right (worst: Non-IID α=0.1, FedMedian)

**Critical Insight**: The heatmap visually confirms that **data heterogeneity** (vertical axis variation) has a larger impact on performance than **aggregation strategy choice** (horizontal axis variation) for most configurations—except FedMedian under extreme Non-IID conditions.

---

# Discussion

## Hypothesis Evaluation Summary

| Hypothesis | Verdict | Evidence |
|-----------|---------|----------|
| **H1**: FedAvg Weighting Advantage | ✅ **CONFIRMED** | +2.01% advantage over FedMean with heterogeneous client sizes (72.98% vs 70.97%) |
| **H2**: Scalability Degradation | ✅ **CONFIRMED** | Systematic degradation: 78.84% → 75.58% → 72.98% for FedAvg (10 → 25 → 50 clients) |
| **H3**: FedMedian Robustness Trade-off | ✅ **CONFIRMED** | Consistent 0.3-10.3% accuracy penalty vs averaging methods; theoretical robustness |

## H1: The Value of Proportional Weighting

Our experiments provide empirical validation that **dataset-size weighting matters** in realistic federated scenarios.

**Key Finding**: When client dataset sizes are heterogeneous (IID-Unequal with 500-1500 sample range), FedAvg's proportional weighting yields +2.01% accuracy advantage over unweighted FedMean.

**Why This Matters**: In real-world federated deployments, client dataset sizes are rarely uniform:

- **Cross-device FL**: User engagement varies wildly (some users interact daily, others weekly)
- **Cross-silo FL**: Institutions have different data collection capacities (large hospitals vs. small clinics)
- **IoT/Edge scenarios**: Sensors may have different operational durations or sampling rates

Our results demonstrate that ignoring dataset size heterogeneity—by using unweighted averaging—incurs a measurable performance penalty. FedAvg's weighting is not merely a theoretical optimization; it provides practical value.

**Contrasting IID-Equal Results**: When all clients have equal dataset sizes (IID-Equal experiments), FedAvg and FedMean perform identically (within 0.4%), confirming the mathematical equivalence:
$$\text{When } n_1 = n_2 = \ldots = n_N: \quad \sum_{i=1}^{N} \frac{n_i}{n} \mathbf{w}_i = \frac{1}{N} \sum_{i=1}^{N} \mathbf{w}_i$$

## H2: The Scalability Challenge

Increasing the number of participating clients systematically degrades model accuracy, even under ideal IID conditions.

**Degradation Rates**:

- **FedAvg**: −5.86% (10 → 50 clients)
- **FedMean**: −7.50% (10 → 50 clients)
- **FedMedian**: −10.27% (10 → 50 clients)

**Root Cause**: With more clients, the global dataset is partitioned into smaller local datasets. Each client performs local training on a smaller data subset, introducing more noise into local updates. When aggregated, this increased noise slows convergence. With fixed communication rounds (20), larger-scale systems have insufficient time to fully converge.

**Practical Implications**:

1. **Communication Budget Trade-off**: Systems with more clients require proportionally more communication rounds to achieve equivalent accuracy
2. **Client Selection Strategies**: Sampling fewer high-quality clients may outperform full participation of many noisy clients
3. **Local Epochs Tuning**: Increasing local epochs for large-scale deployments may partially compensate for smaller local datasets

**FedMedian Scalability Penalty**: FedMedian suffers disproportionately (−10.27% vs −5.86% for FedAvg), suggesting coordinate-wise median is particularly sensitive to gradient noise. This may limit FedMedian's applicability in large-scale deployments.

## H3: Robustness Comes at a Price

FedMedian consistently underperforms averaging methods across all experimental conditions, confirming the robustness-performance trade-off.

**Performance Cost**:

- Accuracy gap: 0.3% to 10.3% below FedAvg (configuration-dependent)
- Scalability penalty: Largest degradation with increasing clients
- Convergence speed: Visibly slower in early rounds (5-15)

**Why FedMedian Underperforms**:

- **Information Loss**: Coordinate-wise median discards information from all but the middle-ranked client update for each parameter
- **Statistical Inefficiency**: For $N$ honest clients, averaging aggregates information from all $N$ updates, while median only uses the central value
- **Gradient Noise Amplification**: Median is less robust to gradient noise than mean, particularly with fewer samples (smaller local datasets)

**When to Use FedMedian**: Despite performance costs, FedMedian provides inherent Byzantine robustness:

- Breakdown point: 50% (tolerates up to $\lfloor N/2 \rfloor$ malicious clients)
- No assumptions about attack strategy required
- Computationally tractable for moderate-scale deployments (<100 clients)

**Recommendation**: Use FedMedian or Multi-Krum only when Byzantine threats are realistic. For honest-but-heterogeneous settings (Non-IID without adversaries), Multi-Krum outperforms FedMedian while FedAvg/FedMean remain most efficient choices.

## Convergence Dynamics

Visual analysis of convergence curves (Figures 1-2) reveals consistent patterns:

1. **Rapid Initial Phase (Rounds 1-10)**: All strategies exhibit steep accuracy gains as models learn coarse-grained features (edges, colors, basic shapes)

2. **Diminishing Returns (Rounds 11-20)**: Accuracy gains slow significantly as models fine-tune decision boundaries. Marginal improvements per round decrease exponentially.

3. **Early Stopping Opportunities**: For communication-constrained deployments:
   - Stopping at round 15: Sacrifice ~2-3% accuracy for 25% communication savings
   - Stopping at round 10: Sacrifice ~5-7% accuracy for 50% communication savings

## Practical Implications

### 1. Aggregation Strategy Selection

Our results provide actionable guidance for practitioners:

**For Homogeneous Client Sizes (Equal Datasets)**:

- **Recommendation**: FedMean
- **Rationale**: Equivalent performance to FedAvg with reduced complexity (no dataset size tracking)
- **Use Cases**: Cross-silo FL with balanced institutional datasets (e.g., hospital consortia with similar patient populations)
- **Advantage**: Simpler implementation, reduced metadata communication, better privacy (dataset sizes not transmitted)

**For Heterogeneous Client Sizes (Unequal Datasets)**:

- **Recommendation**: FedAvg
- **Rationale**: Empirically validated +2.01% accuracy advantage over unweighted averaging
- **Use Cases**: Cross-device FL with variable user engagement (e.g., mobile keyboard prediction, fitness trackers, IoT sensors)
- **Advantage**: Optimal utilization of available data; prevents bias toward small-dataset clients

**For Byzantine-Threat Scenarios**:

- **Recommendation**: Multi-Krum or FedMedian
- **Rationale**: Multi-Krum selects and averages top m clients (Byzantine-robust + Non-IID capable); FedMedian provides 50% breakdown point
- **Use Cases**: FL with untrusted participants, open federated networks, financial fraud detection
- **Trade-off**: Multi-Krum: ~3-6% penalty vs FedAvg on Non-IID data; FedMedian: 1-10% penalty
- **Note**: Standard (single-client) Krum fails on Non-IID data; always use Multi-Krum variant

### 2. Scalability Considerations

System designers must account for the scalability-accuracy trade-off:

**Small-Scale Deployments (10-25 clients)**:

- Expected accuracy: 75-79% (CIFAR-10 baseline)
- Fast convergence: 15-20 rounds typically sufficient
- Low gradient noise: All aggregation methods perform well

**Large-Scale Deployments (50+ clients)**:

- Expected accuracy: 68-73% (CIFAR-10 baseline, 20 rounds)
- Slower convergence: May require 30-50+ rounds for equivalent accuracy
- High gradient noise: FedMedian particularly affected
- **Mitigation strategies**:
  - Increase local epochs (e.g., 10 instead of 5)
  - Increase communication rounds proportionally
  - Implement client sampling (select subset each round)
  - Use adaptive learning rates

### 3. Communication Efficiency

Diminishing returns enable early stopping trade-offs:

**Communication Budget Recommendations**:

- **Constrained budget** (10 rounds): Achieve ~65-70% accuracy, 50% communication savings
- **Moderate budget** (15 rounds): Achieve ~70-75% accuracy, 25% communication savings
- **Full budget** (20 rounds): Achieve ~73-79% accuracy (configuration-dependent)

**Client Participation Strategy**: Our experiments use 100% client participation. In practice, sampling strategies (e.g., 10-20% clients per round) can reduce per-round costs while maintaining convergence, though requiring more total rounds.

### 4. Computational Overhead

**Server-Side Aggregation Complexity**:

- **FedAvg/FedMean**: $O(M \cdot N)$ where $M$ = model parameters, $N$ = clients
  - 50 clients, 61K parameters: ~3M operations, negligible time
- **FedMedian**: $O(M \cdot N \log N)$
  - 50 clients, 61K parameters: ~10M operations, ~15-30% overhead
  - 500 clients: ~50% overhead (may become bottleneck)

**Recommendation**: For deployments >100 clients, consider approximate robust aggregators or hybrid approaches (median on suspicious updates only).

## Key Contribution: Byzantine Robustness ≠ Non-IID Resilience

Our most significant finding is the empirical demonstration that **Byzantine-robust aggregation and non-IID data handling are fundamentally different problems requiring different algorithmic solutions**.

### The Problem

Byzantine fault tolerance assumes that honest clients produce "similar" updates (because they have similar IID data), and adversaries produce "outlier" updates. The goal is to identify and exclude outliers. This assumption breaks down under non-IID data heterogeneity, where *honest* clients naturally produce diverse updates due to different local data distributions.

### Empirical Evidence

| Method | Design Goal | Non-IID α=0.1 | Interpretation |
|--------|-------------|---------------|----------------|
| **Standard Krum** | Exclude adversaries | 10.07% | Catastrophic failure |
| **Multi-Krum** | Exclude adversaries + average | 60.69% | Successful adaptation |
| **FedMedian** | Exclude outliers | 43.72% | Significant failure |
| **FedAvg** | Simple averaging | 66.77% | Baseline (no robustness) |

Standard Krum selects a single "most trustworthy" client—appropriate when honest clients are similar and we want to find the "typical" one. Under non-IID, selecting one specialized client (trained on 1-2 classes) produces a model that fails on the other 8-9 classes.

Multi-Krum succeeds because it **averages** multiple clients, capturing diverse class information while still excluding extreme outliers. This preserves some Byzantine robustness while enabling non-IID handling.

### Theoretical Implications

This finding has implications for federated learning system design:

1. **Threat model matters**: Before selecting an aggregation strategy, practitioners must distinguish between adversarial threats (Byzantine) and data distribution challenges (non-IID)

2. **Robustness is not universal**: Methods designed for one problem may fail catastrophically on the other—Krum's 10% accuracy demonstrates this starkly

3. **Adaptation is possible**: Multi-Krum shows that Byzantine-robust methods can be adapted for heterogeneity through multi-client selection, though this may reduce robustness against sophisticated adversaries

4. **Need for hybrid approaches**: Real deployments may face both challenges simultaneously, requiring methods that address both (e.g., Multi-Krum, or SCAFFOLD + Byzantine detection)

### Connection to Prior Work

Our empirical results align with theoretical observations from recent surveys [@wang2024noniid] that "existing FL methods such as Krum may be broken in extreme non-IID scenarios." We provide the first systematic empirical validation of this claim, quantifying the failure mode (10.07% accuracy) and demonstrating a successful adaptation (Multi-Krum at 60.69%).

---

# Limitations and Future Work

## Limitations

### 1. Communication Rounds and Convergence **✅ Addressed**

**Original Limitation**: We initially used 20 communication rounds (reduced from typical 50-100) to enable rapid iteration across 18 experimental configurations.

**Extended Analysis**: We reran two critical IID configurations with 50 rounds (IID-Equal and IID-Unequal, 50 clients, all 3 aggregators) to validate convergence behavior.

**Findings**:

- Models at 20 rounds reach only 90.7-93.9% of 50-round performance
- All strategies continue improving substantially beyond 20 rounds (+4.70% to +6.96%)
- **However**, relative strategy rankings remain stable (hypotheses validated at both 20r and 50r)
- FedMedian shows slower convergence than averaging methods

**Conclusion**: While 20 rounds underestimates absolute accuracy, it provides valid comparative analysis. Extended convergence analysis confirms our hypothesis validations hold at full convergence.

### 2. Honest Client Assumption

**Limitation**: We evaluate aggregation strategies without Byzantine (malicious) clients. FedMedian's primary value proposition—Byzantine robustness—is not empirically tested.

**Partial Mitigation**: We completed preliminary Level 3 experiments with Byzantine attacks but results are not yet integrated into comprehensive analysis.

**Future Work**: Full Byzantine attack simulation (random noise, sign flipping, backdoor attacks) with varying attacker fractions (10-40%).

### 3. Single Dataset and Architecture

**Limitation**: Experiments limited to:

- **Dataset**: CIFAR-10 (image classification)
- **Model**: SimpleCNN (61K parameters)

**Generalization Concerns**: Results may not transfer to:

- Natural language processing (transformers, BERT-scale models)
- Larger vision models (ResNet-50, ViT)
- Other domains (time-series, tabular data, graphs)

**Rationale**: CIFAR-10 provides standardized benchmark for controlled comparisons while remaining computationally tractable.

### 4. IID vs Non-IID Integration **✅ Fully Addressed**

**Original Limitation**: Level 2 Non-IID experiments were completed but not initially integrated into comparative analysis.

**Current Status**:

- ✅ All Non-IID experiments completed and integrated (α ∈ {0.1, 0.5, 1.0})
- ✅ Multi-Krum successfully implemented and validated (fixing standard Krum's Non-IID failure)
- ✅ Comprehensive comparative analysis with IID experiments included
- ✅ Critical distinction identified: Byzantine robustness vs. Non-IID resilience requires different algorithmic approaches

**Key Finding**: Standard Krum's single-client selection fails catastrophically on Non-IID data (10.07% accuracy at α=0.1), while Multi-Krum (m=48 clients) achieves competitive performance (60.69-69.56%), demonstrating that Byzantine-robust aggregators must be adapted for statistical heterogeneity.

### 5. Communication Cost Modeling

**Limitation**: Evaluation focuses on test accuracy and convergence rounds. True communication costs depend on:

- Bytes transmitted (model size, compression)
- Network latency and bandwidth
- Asynchronous updates and stragglers

**Missing Analysis**:

- Wall-clock time under realistic network conditions
- Communication-computation trade-offs with compression
- Partial client participation per round

### 6. Hyperparameter Sensitivity

**Limitation**: Experiments use fixed hyperparameters:

- Learning rate: 0.01 (no decay)
- Local epochs: 5
- Batch size: 32

**Impact**: Different hyperparameters may alter relative performance of aggregation strategies. Our conclusions hold for this specific configuration but may not generalize to all hyperparameter settings.

**Future Work**: Systematic hyperparameter sweep to assess sensitivity of findings.

### 7. Scope of Aggregation Methods Compared

**Limitation**: This study focuses on fundamental aggregation strategies (FedAvg, FedMean, FedMedian, Krum variants) but does not compare against methods specifically designed for non-IID data:

- **SCAFFOLD** [@karimireddy2020scaffold]: Uses control variates to correct client drift; has shown consistent improvements over FedAvg in heterogeneous settings
- **FedProx** [@li2020federated]: Adds proximal regularization to limit local update divergence
- **FedDC** [@gao2022feddc]: Decouples and corrects local drift; achieves state-of-the-art on CIFAR-10 with Dirichlet non-IID

**Rationale**: Our goal is to establish baselines for fundamental strategies and demonstrate the Byzantine vs. non-IID distinction. Methods like SCAFFOLD and FedProx address client drift through different mechanisms (variance reduction, regularization) and represent a different class of solutions.

**Important Note**: We do not claim optimality for any tested method under non-IID conditions. Multi-Krum's success (60.69%) demonstrates that Byzantine-robust methods can be adapted for heterogeneity, but methods designed specifically for non-IID (SCAFFOLD, FedDC) may perform better. Our contribution is the empirical demonstration that the problem formulation matters: Byzantine robustness ≠ non-IID resilience.

**Future Work**: Comprehensive comparison including SCAFFOLD, FedProx, and FedDC to establish performance hierarchy under various heterogeneity levels.

## Future Research Directions

### Immediate Next Steps

**1. Integration of Non-IID Results** **✅ COMPLETED**

- **Status**: Fully integrated into paper
- **Outcome**: All Level 2 Non-IID results (α ∈ {0.1, 0.5, 1.0}) analyzed and included
- **Key Finding**: FedAvg shows 6.21% degradation with extreme heterogeneity (α=0.1); FedMedian catastrophic failure (43.72%); Multi-Krum competitive (60.69%)
- **Critical Insight**: Byzantine-robust aggregators require multi-client averaging (not single-selection) for Non-IID data

**2. Extended Convergence Analysis** **✅ COMPLETED**

- **Status**: Extended experiments completed (50 rounds for IID-Equal and IID-Unequal)
- **Outcome**: Models at 20 rounds reach only 90.7-93.9% of final performance; relative strategy rankings remain stable
- **Key Finding**: FedMedian exhibits slower convergence than averaging methods; practical implications for communication budgets documented

**3. Byzantine Attack Evaluation**

- **Status**: Some Level 3 experiments completed but not integrated
- **Task**: Systematic Byzantine attack simulation
  - Attack types: Random noise injection, gradient sign flipping, backdoor poisoning
  - Attacker fractions: 10%, 20%, 30%, 40%
  - Defense mechanisms: FedMedian, Multi-Krum, Trimmed Mean
- **Expected Outcome**: Empirical validation of robustness-performance trade-offs
- **Key Question**: At what attacker fraction does robustness justify accuracy penalty?

### Advanced Aggregation Methods

**Adaptive Aggregation**:

- **Client weighting by contribution quality**: Downweight low-loss-reduction clients dynamically
- **Aggregation strategy selection**: Meta-learning to choose FedAvg vs FedMedian per round based on update heterogeneity
- **Momentum-based aggregation**: Server-side momentum to stabilize updates

**Enhanced Robust Aggregation**:

- **Trimmed Mean**: Remove top/bottom k% of updates before averaging (intermediate robustness)
- **Krum variants**: Multi-Krum, Bulyan (combining Krum with trimmed mean)
- **Geometric Median**: Optimal robustness but higher computational cost
- **Centered Clipping**: Clip updates to bounded norm before aggregation

**Personalized Federated Learning**:

- Test whether personalized approaches (local + global model mixture) mitigate scalability degradation
- Clustered FL: Group clients by data similarity, run separate aggregations per cluster
- Per-client learning rate adaptation

### Theoretical Extensions

**Convergence Analysis**:

- Formal convergence rate bounds for FedAvg, FedMean, FedMedian under:
  - IID conditions with heterogeneous client sizes
  - Non-IID label skew
  - Byzantine attacker presence
- Sample complexity requirements for target accuracy

**Optimality Theory**:

- Prove conditions under which FedAvg's weighting is provably optimal
- Characterize robustness-optimality trade-off frontier (Pareto front)

**Privacy Implications**:

- Analyze whether aggregation strategy affects differential privacy guarantees
- Study information leakage through dataset size metadata (FedAvg vs FedMean)

### Generalization Studies

**Datasets**:

- Vision: ImageNet, FEMNIST (naturally non-IID)
- NLP: LSTM sentiment analysis, next-word prediction
- Tabular: UCI benchmarks, healthcare predictions
- Time-series: Activity recognition, sensor forecasting

**Model Architectures**:

- Deeper CNNs (ResNet-18, ResNet-50)
- Transformers (ViT, BERT-base)
- Recurrent networks (LSTM, GRU)

**Domains**:

- Healthcare: Multi-hospital disease prediction, medical imaging
- Finance: Fraud detection across institutions
- IoT: Smart home energy optimization, predictive maintenance

### Real-World Deployment

**Production FL Systems**:

- Deploy on real mobile devices (not simulation)
- Handle network failures, stragglers, partial participation
- Measure wall-clock time, energy consumption, bandwidth usage

**Federated Benchmarks**:

- Participate in community benchmarks (LEAF, FLSim)
- Compare against state-of-the-art baselines
- Contribute findings to FL standardization efforts

---

# Conclusions

This study provides a **multi-dimensional empirical comparison** of fundamental federated learning aggregation strategies—FedAvg, FedMean, and FedMedian—across varying data distributions, client counts, and dataset size heterogeneity. Through 18 experimental configurations on CIFAR-10, we establish empirical baselines and validate key hypotheses about aggregation strategy behavior.

## Key Contributions

**1. Dataset-Size Weighting Matters in Practice**

We empirically validate that FedAvg's proportional weighting provides measurable advantage (+2.01% accuracy) over unweighted FedMean when client dataset sizes are heterogeneous. This confirms that FedAvg's theoretical optimality translates to practical benefit in realistic federated scenarios.

**Implication**: Practitioners should use FedAvg (not FedMean) in cross-device deployments where user engagement varies, or cross-silo settings with heterogeneous institutional data volumes.

**2. Scalability Degrades Performance Systematically**

Increasing client count from 10 → 25 → 50 causes systematic accuracy degradation (−5.86% for FedAvg, −10.27% for FedMedian), even under ideal IID conditions. This scalability penalty arises from increased gradient noise as local datasets shrink.

**Implication**: Large-scale federated systems (100+ clients) require proportionally more communication rounds or enhanced local training to achieve target accuracy.

**3. Robustness Costs 1-10% Accuracy**

FedMedian consistently underperforms averaging methods by 0.3-10.3% across experimental configurations. While providing theoretical Byzantine robustness (50% breakdown point), this performance penalty may be unjustifiable in honest-client scenarios.

**Implication**: Reserve robust aggregators (FedMedian, Multi-Krum, Trimmed Mean) for deployments with realistic Byzantine threats. For heterogeneous-but-honest settings, FedAvg suffices.

**4. Convergence Exhibits Diminishing Returns**

All aggregation strategies show logarithmic convergence with steep initial gains (rounds 1-10) followed by slow refinement (rounds 11-20). This enables early stopping trade-offs: stopping at round 15 sacrifices ~2-3% accuracy while saving 25% communication cost.

**Implication**: Communication-constrained deployments can exploit early stopping without severe accuracy penalties.

## Methodological Contributions

**Systematic Experimental Design**: Our three-dimensional experimental framework (data distribution × client count × aggregation strategy) provides template for future FL aggregation studies.

**Controlled Hypothesis Testing**: By systematically varying one dimension while controlling others (e.g., IID-Equal vs IID-Unequal with same client count), we isolate causal effects of aggregation design choices.

**Reproducible Baselines**: All experiments use standardized configurations (CIFAR-10, SimpleCNN, 20 rounds) with open-source code, enabling direct comparison with future work.

## Practical Decision Framework

We provide actionable guidance for aggregation strategy selection:

| **Deployment Scenario** | **Recommended Strategy** | **Rationale** |
|------------------------|-------------------------|---------------|
| Equal client sizes, IID data | FedMean | Simpler than FedAvg, equivalent performance |
| Heterogeneous client sizes, IID data | FedAvg | +2% accuracy advantage from proportional weighting |
| Small-scale (10-25 clients) | FedAvg or FedMean | All methods perform well (75-79% accuracy) |
| Large-scale (50+ clients) | FedAvg with extended rounds | Best scalability; avoid FedMedian (−10% penalty) |
| Byzantine threats expected | FedMedian or Multi-Krum | Accept 1-10% penalty for robustness |
| Communication-constrained | Any, with early stopping at round 15 | 25% cost savings, ~2-3% accuracy loss |

## Broader Impact

**For FL System Designers**: Our findings quantify trade-offs between aggregation complexity, robustness, and performance—informing architecture decisions for production FL deployments.

**For FL Researchers**: We establish empirical baselines for evaluating novel aggregation strategies. Claims of improved performance should be benchmarked against our FedAvg/FedMean baselines under comparable conditions.

**For ML Practitioners**: The +2% FedAvg advantage with heterogeneous client sizes demonstrates that "implementation details" (dataset-size weighting) can materially impact model quality.

## Final Takeaway

Aggregation strategy selection is **not one-size-fits-all**. The optimal choice depends on deployment characteristics:

- **Data heterogeneity**: IID-Equal → FedMean; IID-Unequal → FedAvg
- **Scale**: Small → any method; Large → FedAvg with careful tuning
- **Threat model**: Honest clients → averaging; Byzantine threats → robust aggregators

Our multi-dimensional study provides the empirical foundation for making these choices systematically rather than heuristically.

---

# References

::: {#refs}
:::

---

# Appendix: Experimental Configuration

## Hardware and Software

| Component | Specification |
|-----------|--------------|
| **Hardware** | |
| GPU | 2× NVIDIA RTX 4090 (24GB VRAM each) |
| CPU | AMD Threadripper (128 cores) |
| RAM | 1 TB DDR4 |
| **Software** | |
| Operating System | Ubuntu 22.04 LTS |
| Python | 3.11 |
| PyTorch | 2.1.0 |
| CUDA | 12.1 |
| Flower | 1.7.0 |
| Ray | 2.9.0 |

## Experimental Execution

**Total Experiments**: 18 configurations

**Execution Time**: Approximately 2-3 hours for complete suite (parallelized across 2× GPUs)

**Resource Utilization**:

- GPU memory: ~2-4GB per experiment (0.04 GPU per client with Ray)
- CPU cores: 20-30 cores active during simulation
- Disk space: ~50MB per experiment (JSON logs + model checkpoints)

## Reproducibility

All experiments were conducted with fixed random seed (42) for reproducibility. Complete source code, configurations, and results are available in the project repository.

### Repository Structure

```
experiments/
├── shared/
│   ├── data_utils.py          # Data partitioning (IID-Equal, IID-Unequal, Non-IID)
│   ├── metrics.py              # MetricsLogger, evaluation functions
│   └── model.py                # SimpleCNN architecture
│
├── level1_fundamentals/
│   ├── run_experiment.py       # Flexible experiment runner (IID experiments)
│   ├── run_fedavg.py           # Legacy FedAvg implementation
│   ├── run_fedmean.py          # FedMean implementation
│   ├── run_fedmedian.py        # FedMedian implementation
│   ├── analyze_comprehensive_results.py  # Generates comparison plots
│   └── results/
│       └── comprehensive/      # JSON experiment logs
│
├── level2_heterogeneous/
│   ├── run_fedavg.py           # FedAvg with Non-IID data
│   ├── run_fedmedian.py        # FedMedian with Non-IID data
│   ├── run_krum.py             # Krum with Non-IID data
│   └── results/
│       └── comprehensive/      # Non-IID experiment logs (not integrated)
│
└── run_comprehensive_experiments.sh  # Master execution script
```

### Key Execution Scripts

**Master Script**: `run_comprehensive_experiments.sh`

- Orchestrates all 18 experiments
- Handles environment activation (conda)
- Logs all output to `comprehensive_experiments.log`

**Analysis Script**: `analyze_comprehensive_results.py`

- Loads JSON experiment results from `./results/comprehensive/`
- Generates 3 comparison plots (Figures 1-3)
- Exports `comprehensive_summary.csv`
- Performs hypothesis testing

### Data Files in Paper Directory

This paper directory contains all evidence files:

```
papers/federated-aggregation-comparison/
├── federated_learning_aggregation_comparison.qmd  # This paper
├── comprehensive_summary.csv         # Summary table
├── comparison_iid_equal_vs_unequal.png  # Figure 1: IID-Equal vs IID-Unequal
├── comparison_client_scaling.png     # Figure 2: Client Scaling Analysis
├── convergence_20_vs_50_rounds.png   # Figure 3: Extended Convergence (20 vs 50 rounds)
├── noniid_comprehensive_analysis.png # Figure 4: Non-IID Analysis
├── iid_vs_noniid_comparison.png      # Figure 5: IID vs Non-IID Direct Comparison
├── grand_heatmap_all_experiments.png # Figure 6: Comprehensive Heatmap (all experiments)
├── krum_standard_vs_multi_comparison.png  # Figure 7: Standard vs Multi-Krum
├── krum_convergence_comparison.png   # Figure 8: Krum Convergence Trajectories
├── generate_krum_comparison.py       # Script to generate Krum figures
├── references.bib                    # BibTeX citations
├── KRUM_FIX_SUMMARY.md              # Multi-Krum implementation fix documentation
└── README.md                         # Experimental documentation
```

### Running Experiments from Scratch

```bash
# 1. Set up environment
cd /path/to/CAAC-FL/experiments
conda activate caac-fl

# 2. Run comprehensive suite (all 18 experiments)
./run_comprehensive_experiments.sh

# 3. Generate analysis plots and summary
cd level1_fundamentals
python analyze_comprehensive_results.py

# 4. Render paper
cd ../papers/federated-aggregation-comparison
quarto render federated_learning_aggregation_comparison.qmd
```

**Expected Runtime**: ~2-3 hours for complete experimental suite + analysis

---

**Acknowledgments**: This research was conducted using the CAAC-FL (Communication-Aware Adaptive Clustering for Federated Learning) experimental framework. We thank the developers of Flower (flwr) and Ray for providing robust federated learning simulation infrastructure.
